---
title: manage-boot-images
authors:
  - "@djoshy"
reviewers: 
  - "@yuqi-zhang"
  - "@mrunal"
  - "@cgwalters, for rhcos context" 
  - "@joelspeed, for machine-api context" 
  - "@sdodson, for installer context"
approvers:
  - "@yuqi-zhang"
api-approvers: 
  - "@joelspeed"
creation-date: 2023-10-16
last-updated: 2022-12-11
tracking-link:
  - https://issues.redhat.com/browse/MCO-589
see-also:
replaces: 
  - https://github.com/openshift/enhancements/pull/368
superseded-by: 
  - https://github.com/openshift/enhancements/pull/201
---

# Managing boot images via the MCO

## Summary

This is a proposal to manage bootimages via the `Machine Config Operator`(MCO), leveraging some of the [pre-work](https://github.com/openshift/installer/pull/4760) done as a result of the discussion in [#201](https://github.com/openshift/enhancements/pull/201). This feature will only target standalone OCP installs. It will also be user opt-in and is planned to be released behind a feature gate.

For `MachineSet` managed clusters, the end goal is to create an automated mechanism that can:
- update the boot images references in `MachineSets` to the latest in the payload image
- ensure stub ignition referenced in each `Machinesets` is in spec 3 format

For clusters that are not managed by `MachineSets`, the end goal is to create a document(KB or otherwise) that a cluster admin would follow to update their boot images.


## Motivation

Currently, bootimage references are [stored](https://github.com/openshift/installer/blob/1ca0848f0f8b2ca9758493afa26bf43ebcd70410/pkg/asset/machines/gcp/machines.go#L204C1-L204C1) in a `MachineSet` by the openshift installer during cluster bringup and is thereafter not managed. These boot image references are not updated on an upgrade, so any node scaled up using it will boot up with the original “install” bootimage. This has caused a myriad of issues during scale-up due to this version skew, when the nodes attempt the final pivot to the release payload image. Issues linked below:
- Afterburn [[1](https://issues.redhat.com/browse/OCPBUGS-7559)],[[2](https://issues.redhat.com/browse/OCPBUGS-4769)]
- podman [[1](https://issues.redhat.com/browse/OCPBUGS-9969)]
- skopeo [[1](https://issues.redhat.com/browse/OCPBUGS-3621)]

Additionally, the stub secret [referenced](https://github.com/openshift/installer/blob/1ca0848f0f8b2ca9758493afa26bf43ebcd70410/pkg/asset/machines/gcp/machines.go#L197) in the `MachineSet` is also not managed. This stub is used by the ignition binary in firstboot to auth and consume content from the `machine-config-server`(MCS). The content served includes the actual ignition configuration and the target OCI format RHCOS image. The ignition binary now does first boot provisioning based on this, then hands off to the `machine-config-daemon`(MCD) first boot service to do the reboot into the target OCI format RHCOS image. 

There has been [a previous effort](https://github.com/openshift/machine-config-operator/pull/1792) to manage the stub secret. It was [reverted](https://github.com/openshift/machine-config-operator/pull/2126) and then [brought back](https://github.com/openshift/machine-config-operator/pull/2827#issuecomment-996156872) just for bare metal clusters. For other platforms, the `*-managed` stub secrets still get generated by the MCO, but are not injected into the `MachineSet`. The proposal plans to utilize these unused `*-managed` stub secrets, but it is important to note that this stub secret is generated(and synced) by the MCO and will ignore/override any user customizations to the stub secret. This limitation will be mentioned in the documentation, and a later release will provide support for user customization of the stub secret, either via API or a workaround thorugh additional documentation. This should not be an issue for the majority of users as they very rarely customize the stub secret.

In certain long lived clusters, the MCS TLS cert contained within the above ignition configuration may be out of date. Example issue [here](https://issues.redhat.com/browse/OCPBUGS-1817). While this has been partly solved [MCO-642](https://issues.redhat.com/browse/MCO-642) (which allows the user to manually rotate the cert) it would be very beneficial for the MCO to actively manage this TLS cert and take this concern away from the user.

### User Stories

* As an Openshift engineer, having nodes boot up on an unsupported OCP version is a security liability. By having nodes directly boot on the release payload image, it helps me avoid tracking incompatibilities across OCP release versions and shore up technical debt(see issues linked above). 

* As a cluster administrator, having to keep track of a "boot" vs "live" image for a given cluster is not intuitive or user friendly. In the worst case scenario, I will have to reset a cluster(or do a lot of manual steps with rh-support in recovering the node) simply to be able to scale up nodes after an upgrade. If I'm managing a `MachineSet` managed cluster, once opted in, this feature will be a "switch on and forget" mechanism for me. If I'm managing a non `Machineset` managed cluster, this would provide me with documentation that I could follow after an upgrade to ensure my cluster has the latest bootimages.

### Goals

The MCO will take over management of the boot image references and the stub ignition. The installer is still responsible for creating the `MachineSet` at cluster bring-up of course, but once cluster installation is complete the MCO will ensure that boot images are in sync with the latest payload. From the user standpoint, this should cause less compatibility issues as nodes will no longer need to pivot to a different version of rhcos during node scaleup.

This should not interfere with existing workflows such as Hive and ArgoCD. As this is an opt-in mechanism, the cluster admin will be protected against such scenarios of accidental "reconciliation". 

### Non-Goals

- The new subcontroller is only intended to support clusters that use MachineSet backed node scaling. This is meant to be a user opt-in feature, and if the user wishes to keep their boot images static it will let them do so.
- This does not intend to solve [booting into custom pools](https://issues.redhat.com/browse/MCO-773). 
- This does not target Hypershift, as [it does not use machinesets](https://github.com/openshift/hypershift/blob/32309b12ae6c5d4952357f4ad17519cf2424805a/hypershift-operator/controllers/nodepool/nodepool_controller.go#L2168).
- This does not target [ControlPlaneMachineSets](https://docs.openshift.com/container-platform/4.14/machine_management/control_plane_machine_management/cpmso-about.html).

## Proposal

__Overview__

- The `machine-config-controller`(MCC) pod will gain a new sub-controller `machine_set_boot_image_controller`(MSBIC) that monitors `MachineSet` changes and the `coreos-bootimages` [ConfigMap](https://github.com/openshift/installer/pull/4760) changes.
- Before processing a MachineSet, the MSBIC will check if the following conditions are satisfied:
  - `ManagedBootImages` feature gate is active
  - The cluster and/or the machineset is opted-in to boot image updates.
  - The golden configmap is verified to be in sync with the current version of the MCO. The MCO will "stamp"(annotate) the golden configmap with the new version of the MCO after atleast 1 node has succesfully completed an update to the new OCP image. This helps prevent `machinesets` being updated too soon at the end of a cluster upgrade, before the MCO itself has updated and has had a chance to roll out the new OCP image to the cluster. 

  If any of the above checks fail, the MSBIC will exit out of the sync.
- Based on platform and architecture type, the MSBIC will check if the boot images referenced in the `providerSpec` field of the `MachineSet` is the same as the one in the ConfigMap. Each platform(gcp, aws...and so on) does this differently, so this part of the implementation will have to be special cased. The ConfigMap is considered to be the golden set of bootimage values, i.e. they will never go out of date. If it is not a match, the `providerSpec` field is cloned and updated with the new boot image reference.
- Next, it will check if the stub secret referenced within the `providerSpec` field of the `MachineSet` is managed i.e. `worker-user-data-managed` and not `worker-user-data`. If it is unmanaged, the cloned `providerSpec` will be updated to reference the managed stub secret. This step is platform/arch agnostic.

- Finally, the MSBIC will attempt to patch the `MachineSet` if required. Failure to do so will cause a degrade. 

#### Degrade Mechanism

The MSBIC will degrade the worker `MachineConfigPool` via a new [MachineConfigPoolConditionType](https://github.com/openshift/api/blob/master/machineconfiguration/v1/types.go#L492). This would be an API change, but a fairly simple one is it only adding a new condition type. The node controller(another sub controller within the MCC) would then [check for this condition](https://github.com/openshift/machine-config-operator/blob/master/pkg/controller/node/status.go#L142C34-L142C34) and degrade the worker pool, effectively degrading the operator.

As mentioned in the above section, degrading will only happen when the patching of the MachineSet fails. This is likely due to a temporary API server outage and will resolve itself without user intervention. The degrade condition is calculated at the end of a sync loop. In the case of multiple such failures within a single sync loop, the message for the degrade will be accumulated to include the `MachineSets` associated with all the failures. 

#### Reverting to original bootimage

The proposal will introduce a CR, `BootImageHistory` to store the boot image history associated with a given machineset. By providing this CR and accompanying documentation, the user will be able to restore their machinesets to an earlier state if they wish to do so. 

### Workflow Description

It is important to note that there would be two "opt-in" knobs while this feature is under TechPreview. The user would first have to turn on the feature gate, and then the opt-in mechanism. The secondary knob is necessary as some customers may want to keep their boot images static when this feature leaves TechPreview.

See the API extension section for examples of how this feature can be turned on and off. 

#### Variation and form factor considerations [optional]

Any form factor using the MCO and `MachineSets` will be impacted by this proposal. So case by case:
- Standalone OpenShift: Yes, this is the main target form factor.
- microshift: No, as it does [not](https://github.com/openshift/microshift/blob/main/docs/contributor/enabled_apis.md) use `MachineSets`.
- Hypershift: No, Hypershift does not have this issue.

##### Supported platforms

The initial release(phase 0) will support GCP. In future releases, we will add in support for remaining platforms as we gain confidence in the functionality and understand the specific needs of those platforms. For platforms that cannot be supported, we aim to atleast provide documentation to perform the boot image updates manually. Here is an exhaustive list of all the platforms:

- gcp
- aws
- azure
- alibabacloud
- nutanix
- powervs
- openstack
- vsphere
- baremetal
- libvirt
- ovirt
- ibmcloud

This work will be tracked in [MCO-793](https://issues.redhat.com/browse/MCO-793).

##### Cluster API backed machinesets

As the Cluster API move is impending(initial release in 4.16 and default-on release in 4.17), it is necessary that this enhancement plans for the changes required in an CAPI backed cluster. Here are a couple of sample YAMLs used in CAPI backed `Machinesets`, from the [official Openshift documentation](https://docs.openshift.com/container-platform/4.14/machine_management/capi-machine-management.html#capi-sample-yaml-files-gcp).

###### MachineSet resource
```
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineSet
metadata:
  name: <machine_set_name> 
  namespace: openshift-cluster-api
spec:
  clusterName: <cluster_name> 
  replicas: 1
  selector:
    matchLabels:
      test: test
  template:
    metadata:
      labels:
        test: test
    spec:
      bootstrap:
         dataSecretName: worker-user-data 
      clusterName: <cluster_name> 
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: GCPMachineTemplate 
        name: <machine_set_name> 
      failureDomain: <failure_domain> 
```
###### GCPMachineTemplate
```
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: GCPMachineTemplate 
metadata:
  name: <template_name> 
  namespace: openshift-cluster-api
spec:
  template:
    spec: 
      rootDeviceType: pd-ssd
      rootDeviceSize: 128
      instanceType: n1-standard-4
      image: projects/rhcos-cloud/global/images/rhcos-411-85-202203181601-0-gcp-x86-64
      subnet: <cluster_name>-worker-subnet
      serviceAccounts:
        email: <service_account_email_address>
        scopes:
          - https://www.googleapis.com/auth/cloud-platform
      additionalLabels:
        kubernetes-io-cluster-<cluster_name>: owned
      additionalNetworkTags:
        - <cluster_name>-worker
      ipForwarding: Disabled
```
As can be seen, the bootimage becomes part of an `InfrastructureMachineTemplate` object (eg a GCPMachineTemplate), and then the MachineSet references this template and creates new machines from the template. The stub secret is now stored in a `bootstrap` object. Unlike MAPI backed MachineSets, both of them are no longer part of a single `providerSpec` object. 

It is important to note that InfrastructureMachineTemplate is different per platform and is immutable. This will prevent an update in place style approach and would mean that the template would need to be cloned, updated during the clone, and then the MachineSet updated. This is somewhat similar to the approach used in the current MAPI PoC of cloning the `providerSpec` object, updating it and then patching the `MachineSet`. The `bootstrap` object is platform agnostic, making it somewhat simpler to update. 

Based on the observation above, here is a rough outline of what CAPI support would require:
- CAPI backed MachineSet detection, so the MSBIC knows when to invoke the CAPI path
- If a boot image update is required, create a new `InfrastructureMachineTemplate` by cloning the existing and updating the boot image reference within. The name of the new `InfrastructureMachineTemplate` object will be generated by hashing the template content. This is consistent with the current CAPI approach to naming new objects.
- Updating the ignition stub in `bootstrap.dataSecretName` to the managed stub secret(`*-managed`) if needed.
- CAPI backed MachineSet patching

Much of the existing design regarding architecture & platform detection, opt-in, degradation and storing boot image history can remain the same. 

When [MachineDeployments](https://cluster-api.sigs.k8s.io/developer/architecture/controllers/machine-deployment#machinedeployment) are introduced into CAPI, this mechanism will need to be reworked to update those rather than the MachineSet itself.

### API Extensions

#### Opt-in Mechanism

This proposal will introduce a discriminated union in [operator types](https://github.com/openshift/api/blob/master/operator/v1/types_machineconfiguration.go) for the MCO, `ManagedBootImageConfig` which has two fields:

- `Mode` This is a string enum which can have three values:
  - `Enabled` - All `Machinesets` will be enrolled for boot image updates.
  - `CustomConfig` - `Machinesets` matched with the label selector will be enrolled for boot image updates.
  - `Disabled` - No `Machinesets` will be enrolled for boot image updates.
- `CustomConfig` This is struct which encloses a label selector that will be used by machineset objects to opt-in. 

Here are some YAML examples that describes operators in each of these modes:
##### Enabled
```
apiVersion: operator.openshift.io/v1
kind: MachineConfiguration
metadata:
  name: default
  labels:
spec:
  managedBootImageConfig:
    mode: Enabled
```
##### Disabled
```
apiVersion: operator.openshift.io/v1
kind: MachineConfiguration
metadata:
  name: default
  labels:
spec:
  managedBootImageConfig:
    mode: Disabled
```
##### MatchSelector
```
apiVersion: operator.openshift.io/v1
kind: MachineConfiguration
metadata:
  name: default
  labels:
spec:
  managedBootImageConfig:
    mode: CustomConfig
    CustomConfig:
      machineSetSelector:
        matchLabels:
          machineconfiguration.openshift.io/mco-managed-machineset: ""        
```
Note: While in this mode, the label added to the selector will have to be added to the `machineset` object.

A [ValidatingAdmissionPolicy](https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/) will be implemented via an MCO manifest that will restrict updating the `ManagedBootImageConfig` object to only supported platforms(initially, just GCP). This will be updated as we phase in support for other platforms. Here is a sample policy that would do this:

```
apiVersion: admissionregistration.k8s.io/v1beta1
kind: ValidatingAdmissionPolicy
metadata:
  name: "managed-bootimages-platform-check"
spec:
  failurePolicy: Fail
  paramKind:
    apiVersion: config.openshift.io/v1
    kind: Infrastructure
  matchConstraints:
    resourceRules:
    - apiGroups:   ["operator"]
      apiVersions: ["v1"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["MachineConfiguration"]
  validations:
    - expression: "has(object.spec.MachineBootImageConfig) && param.status.platformStatus.Type != `GCP`"
      message: "This feature is only supported on these platforms: GCP"
```
This would need an accompanying binding:
```
apiVersion: admissionregistration.k8s.io/v1beta1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "managed-bootimages-platform-check-binding"
spec:
  policyName: "managed-bootimages-platform-check"
  validationActions: [Deny]
  paramRef:
    name: "cluster"
    namespace: "default"
```
#### Tracking boot image history

This proposal will also introduce a new CR, `BootImageHistory` for tracking boot image history. As a starting point, here is a stub type definition for this:

```
type BootImageHistory struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   BootImageHistorySpec   `json:"spec,omitempty"`
	Status BootImageHistoryStatus `json:"status,omitempty"`
}

// BootImageHistorySpec defines the desired state of BootImageHistory
type BootImageHistorySpec struct {
}

// BootImageHistoryStatus defines the observed state of BootImageHistory
type BootImageHistoryStatus struct {
	// machineResourceReference contains identifying information of the machine management resource being tracked.
	// +kubebuilder:validation:Required
	// +required
	MachineResourceReference MachineResourceReference `json:"machineResourceReference"`
	// details is a list of boot image history entries of the machine resource.
	// +optional
	Details []BootImageHistoryDetail `json:"details,omitempty"`
}

type MachineResourceReference struct {
	// name is the machine management resource's name
	// +kubebuilder:validation:Required
	// +required
	Name string `json:"name"`
	// kind is the machine management resource's kind
	// +kubebuilder:validation:Required
	// +required
	Kind string `json:"kind"`
	// apiGroup is name of the APIGroup that the machine management resource belongs to. This is for disambiguating
	// between Cluster API and Machine API backed resources.
	// +kubebuilder:validation:Required
	// +required
	APIGroup string `json:"apiGroup"`
}

// BootImageHistoryDetail is the struct for each element in the Details array
type BootImageHistoryDetail struct {
	// updateTime records the timestamp at which the update took place.
	// +required
	UpdateTime metav1.Time `json:"updatedTime"`
	// bootImageRef records the new boot image reference to which the update took place.
	// +required
	BootImageRef string `json:"bootImageRef"`
}

// BootImageHistoryList contains a list of BootImageHistory
type BootImageHistoryList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []BootImageHistory `json:"items"`
}

```
There will be one instance of this per machine management resource(which can be a MachineSet[MAPI or CAPI], MachineDeployment...etc). It will be named the same as the resource being tracked. The MSBIC is responsible for creating and updating this CR when a boot image update takes place. This CR will exist in the same namespace as the resource.

YAML Example for a MAPI backed machineset scenario:
```
apiVersion: machineconfiguration.openshift.io/v1alpha1
kind: BootImageHistory
metadata:
  name: djoshy10-2tcqv-worker-a
spec: {}
status:
  machineResourceReference:
    name: djoshy10-2tcqv-worker-a
    kind: MachineSet
    apiGroup: cluster.x-k8s.io/v1alpha3
  details:
    - updateTime: "2023-12-14T12:00:00Z"
      bootImageRef: "projects/rhcos-cloud/global/images/rhcos-414-92-202308032115-0-gcp-x86-64"
    - updateTime: "2023-12-14T14:30:00Z"
      bootImageRef: "projects/rhcos-cloud/global/images/rhcos-415-92-202311241643-0-gcp-x86-64"

```

YAML Example for a CAPI backed machineset scenario:
```
apiVersion: machineconfiguration.openshift.io/v1alpha1
kind: BootImageHistory
metadata:
  name: djoshy10-2tcqv-worker-a
spec: {}
status:
  machineResourceReference:
    name: djoshy10-2tcqv-worker-a
    kind: MachineSet
    apiGroup: machine.openshift.io/v1beta1
  details:
    - updateTime: "2023-12-14T12:00:00Z"
      bootImageRef: "projects/rhcos-cloud/global/images/rhcos-414-92-202308032115-0-gcp-x86-64"
    - updateTime: "2023-12-14T14:30:00Z"
      bootImageRef: "projects/rhcos-cloud/global/images/rhcos-415-92-202311241643-0-gcp-x86-64"

```
The goal of this is to provide information about the "lineage" of a machine management resource to the user. The user can then manually restore their machine management resource to an earlier state if they wish to do so by following documentation. The MCO will not directly consume from this CR. This is not planned to be part of the initial release, but more of a nice to have.

### Implementation Details/Notes/Constraints [optional]

![Sub Controller Flow](manage_boot_images_flow.jpg)

![MachineSet Reconciliation Flow](manage_boot_images_reconcile_loop.jpg)

The implementation has a GCP specific POC here:
- https://github.com/openshift/machine-config-operator/pull/3980

### Risks and Mitigations

The biggest risk in this enhancement would be delivering a bad boot image. To mitigate this, we have outlined a revert option.

How will security be reviewed and by whom? TBD
This is a solution aimed at reducing usage of outdated artifacts and should not introduce any security concerns that do not currently exist. 

How will UX be reviewed and by whom? TBD 
The UX element involved include the user opt-in and opt-out, which is currently up for debate. 

### Drawbacks

TBD, based on the open questions below.

## Design Details

### Open Questions

### Test Plan

In addition to unit tests, the enhancement will also ship with e2e tests, outlined [here](https://issues.redhat.com/browse/MCO-774).

### Graduation Criteria

#### Dev Preview -> Tech Preview

- Support for GCP
- Opt-in and Degrade mechanism
- GCP specific E2E tests
- Feedback from openshift teams
- UPI documentation based on IPI workflow for select platforms
- [Good CI signal from autoscaling nodes](https://github.com/cgwalters/enhancements/blob/5505d7db7d69ffa1ee838be972c70b572d882891/enhancements/bootimages.md#test-plan)


#### Tech Preview -> GA

- Feedback from interested customers
- User facing documentation created in [openshift-docs](https://github.com/openshift/openshift-docs/)

Additionaly, a phased approach such as the following is the proposed:

#### Phase 0
- Support for GCP
- vsphere UPI documentation
- Opt-in mechanism
- Degrade mechanism
- E2E tests

#### Phase 1
- Support for Azure and AWS
- MCS TLS cert management

#### Phase 2
- Tracking boot image history
- User facing documentation for manual restoration
- User customization of ignition stub

#### Removing a deprecated feature

This does not remove an existing feature.

### Upgrade / Downgrade Strategy

__Upgrade__

This mechanism is only active shortly after an upgrade, which is when the ConfigMap containing the bootimages are updated by the CVO manifest. It will also run during machineset edits but patching will only occur if there is a mismatch in bootimages.

__Downgrade__

- If the cluster is downgrading to a version that supports this feature, the boot images will track the downgraded version.
- If the cluster is downgrading to a version that does not support this feature, the boot images will not track to the downgraded version. So, it may be wise to opt-out of the feature prior to the downgrade if "normal(i.e. older) OCP behavior" is expected. 

### Version Skew Strategy

N/A

### Operational Aspects of API Extensions

TBD, based on how the opt-in feature would work.

#### Failure Modes

TBD

#### Support Procedures

TBD

## Implementation History

TBD

## Alternatives

TBD
